# AI-Based Linkage System - Improvement Roadmap
## Environmental Bowtie Risk Analysis Application

**Current Version**: 3.0 (Production)
**Document Version**: 1.0
**Last Updated**: 2025-12-28

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Current System Analysis](#current-system-analysis)
3. [Improvement Categories](#improvement-categories)
4. [Priority Roadmap](#priority-roadmap)
5. [Implementation Details](#implementation-details)
6. [Success Metrics](#success-metrics)

---

## Executive Summary

The AI-powered vocabulary linkage system has achieved production readiness with comprehensive features. This document outlines strategic improvements across 6 categories to enhance accuracy, performance, user experience, and domain intelligence.

**Key Priorities:**
- üéØ P0: User feedback learning system (highest impact)
- üöÄ P1: Performance optimizations and caching
- üß† P2: Advanced NLP and machine learning
- üåç P3: Domain knowledge expansion
- üìä P4: Analytics and visualization
- üîß P5: Configuration and customization

---

## Current System Analysis

### ‚úÖ Strengths

**Multi-Method Analysis:**
- Jaccard & cosine semantic similarity
- 10 environmental keyword themes
- Causal relationship detection with 5 patterns
- Domain-specific rules for environmental pathways

**Production-Ready Architecture:**
- Graceful degradation (works without optional packages)
- Comprehensive error handling
- Modular, reusable functions
- Well-documented code (1,556 lines)

**Environmental Domain Expertise:**
- 10 thematic areas (water, pollution, ecosystems, fisheries, etc.)
- 5 process chains (pollution, ecosystem, water quality, climate, fisheries)
- Activity‚ÜíPressure‚ÜíConsequence pathway detection
- Control intervention analysis

**User Experience:**
- Real-time suggestions in guided workflow
- Visual similarity indicators
- Explanatory reasoning for each suggestion
- One-click acceptance

### ‚ö†Ô∏è Areas for Improvement

**Algorithm Limitations:**
- No machine learning adaptation
- Limited to English text analysis
- No temporal or contextual analysis
- Static thresholds (not user-adjustable)

**Performance Constraints:**
- No caching of computed similarities
- Full recomputation on each analysis
- No indexing for large vocabularies
- Sequential processing (no parallelization)

**User Feedback Gap:**
- No learning from user selections
- Cannot improve suggestions over time
- No confidence scoring
- Missing acceptance/rejection tracking

**Domain Knowledge:**
- Fixed environmental rules (not expandable)
- No regional/geographic specialization
- Limited to predefined themes
- No custom rule creation

**Analytics Deficit:**
- No usage statistics
- Cannot identify weak suggestions
- No A/B testing capability
- Missing performance benchmarking

---

## Improvement Categories

### üéØ P0: User Feedback & Learning System

**Objective**: Learn from user behavior to improve suggestion quality over time

#### I-001: Suggestion Acceptance Tracking
**Priority**: P0 (Critical)
**Effort**: Medium (2-3 weeks)
**Impact**: Very High

**Description:**
Track which AI suggestions users accept vs. reject to identify effective patterns.

**Implementation:**
```r
# New reactive value in workflow
suggestion_feedback <- reactiveVal(data.frame(
  timestamp = character(),
  suggestion_id = character(),
  from_type = character(),
  to_type = character(),
  similarity = numeric(),
  method = character(),
  action = character(),  # "accepted", "rejected", "ignored"
  user_id = character(),
  session_id = character(),
  stringsAsFactors = FALSE
))

# Track acceptance
observeEvent(input$suggestion_clicked_*, {
  log_suggestion_feedback(
    suggestion = suggestion_data,
    action = "accepted",
    user_id = session$user,
    session_id = session$token
  )
})

# Track rejection (new "Dismiss" button)
observeEvent(input$dismiss_suggestion_*, {
  log_suggestion_feedback(
    suggestion = suggestion_data,
    action = "rejected",
    user_id = session$user,
    session_id = session$token
  )
})
```

**Database Schema:**
```sql
CREATE TABLE suggestion_feedback (
  id INTEGER PRIMARY KEY,
  timestamp DATETIME,
  from_id VARCHAR(50),
  from_name TEXT,
  from_type VARCHAR(50),
  to_id VARCHAR(50),
  to_name TEXT,
  to_type VARCHAR(50),
  similarity REAL,
  method VARCHAR(50),
  action VARCHAR(20),
  user_id VARCHAR(50),
  session_id VARCHAR(100),
  context_data TEXT  -- JSON with additional context
);

CREATE INDEX idx_feedback_action ON suggestion_feedback(action);
CREATE INDEX idx_feedback_method ON suggestion_feedback(method);
CREATE INDEX idx_feedback_similarity ON suggestion_feedback(similarity);
```

**Benefits:**
- Identify high-quality vs. low-quality suggestions
- Adjust similarity thresholds based on acceptance rates
- Personalize suggestions per user (if multi-user)
- Generate training data for ML models

**Success Metrics:**
- Suggestion acceptance rate improvement (target: >60%)
- Reduced "No suggestions" scenarios
- Faster workflow completion times

---

#### I-002: Adaptive Similarity Thresholds
**Priority**: P0 (Critical)
**Effort**: Medium (2 weeks)
**Impact**: High

**Description:**
Dynamically adjust similarity thresholds based on user feedback patterns.

**Implementation:**
```r
# Adaptive threshold calculator
calculate_adaptive_threshold <- function(link_type, method, feedback_data) {
  # Get historical acceptance data for this link type + method
  relevant_feedback <- feedback_data %>%
    filter(
      paste(from_type, to_type, sep = "_") == link_type,
      method == !!method,
      action %in% c("accepted", "rejected")
    )

  if (nrow(relevant_feedback) < 20) {
    # Not enough data, use default
    return(0.3)
  }

  # Calculate acceptance rate by similarity bands
  bands <- relevant_feedback %>%
    mutate(
      similarity_band = cut(similarity,
                           breaks = seq(0, 1, 0.1),
                           labels = FALSE)
    ) %>%
    group_by(similarity_band) %>%
    summarise(
      acceptance_rate = mean(action == "accepted"),
      n = n(),
      .groups = 'drop'
    )

  # Find optimal threshold (>70% acceptance rate)
  optimal_band <- bands %>%
    filter(acceptance_rate >= 0.7, n >= 5) %>%
    arrange(similarity_band) %>%
    slice(1)

  if (nrow(optimal_band) == 0) {
    return(0.3)  # Fallback to default
  }

  # Convert band to threshold
  threshold <- (optimal_band$similarity_band - 1) * 0.1

  # Ensure reasonable bounds
  threshold <- max(0.2, min(0.7, threshold))

  return(threshold)
}

# Use adaptive thresholds in main function
find_vocabulary_links <- function(vocabulary_data,
                                 similarity_threshold = "adaptive",  # New option
                                 feedback_data = NULL,
                                 ...) {

  if (similarity_threshold == "adaptive" && !is.null(feedback_data)) {
    # Calculate adaptive thresholds per link type
    adaptive_thresholds <- list(
      activity_pressure = calculate_adaptive_threshold(
        "Activity_Pressure", "causal", feedback_data
      ),
      pressure_consequence = calculate_adaptive_threshold(
        "Pressure_Consequence", "causal", feedback_data
      ),
      control_pressure = calculate_adaptive_threshold(
        "Control_Pressure", "causal_intervention", feedback_data
      )
    )

    cat("Using adaptive thresholds:\n")
    print(adaptive_thresholds)
  } else {
    # Use fixed threshold
    adaptive_thresholds <- list(
      activity_pressure = similarity_threshold,
      pressure_consequence = similarity_threshold,
      control_pressure = similarity_threshold
    )
  }

  # Continue with linking using adaptive thresholds...
}
```

**Benefits:**
- Automatic improvement without manual tuning
- Reduces false positives (irrelevant suggestions)
- Increases true positives (missed good suggestions)
- Adapts to different user preferences

**Success Metrics:**
- Precision increase: >80% (accepted / shown)
- Recall increase: Track user manual additions after rejecting suggestions

---

#### I-003: Confidence Scoring System
**Priority**: P0 (Critical)
**Effort**: Small (1 week)
**Impact**: High

**Description:**
Add confidence scores to suggestions based on multiple factors beyond similarity.

**Implementation:**
```r
# Enhanced suggestion scoring
calculate_confidence_score <- function(suggestion, context, feedback_data = NULL) {

  # Base score from similarity
  score <- suggestion$similarity

  # Factor 1: Method reliability (from historical data)
  if (!is.null(feedback_data)) {
    method_acceptance <- feedback_data %>%
      filter(method == suggestion$method, action == "accepted") %>%
      nrow() / max(1, nrow(filter(feedback_data, method == suggestion$method)))

    score <- score * (0.7 + 0.3 * method_acceptance)
  }

  # Factor 2: Source item confidence
  # If from_item was also an AI suggestion that was accepted, boost confidence
  source_is_ai <- "AI Suggestion" %in% context$source_types
  if (source_is_ai) {
    score <- score * 1.1  # 10% boost
  }

  # Factor 3: Multiple connection paths
  # If there are multiple ways to reach this suggestion, boost confidence
  connection_count <- context$connection_paths
  if (connection_count > 1) {
    score <- score * (1 + 0.05 * min(connection_count - 1, 5))
  }

  # Factor 4: Causal chain completeness
  if (grepl("chain", suggestion$method)) {
    score <- score * 1.15  # 15% boost for complete chains
  }

  # Factor 5: Domain-specific rules
  if (grepl("domain", suggestion$method)) {
    score <- score * 1.1  # 10% boost for domain rules
  }

  # Factor 6: Vocabulary coverage
  # Prefer suggestions that increase vocabulary diversity
  vocab_types_used <- unique(c(context$from_types, context$to_types))
  if (suggestion$to_type %in% vocab_types_used) {
    # Already have this type
    score <- score * 0.95
  } else {
    # New type - boost
    score <- score * 1.05
  }

  # Normalize to 0-1
  confidence <- min(1, score)

  # Categorize confidence level
  level <- case_when(
    confidence >= 0.85 ~ "very_high",
    confidence >= 0.70 ~ "high",
    confidence >= 0.50 ~ "medium",
    TRUE ~ "low"
  )

  return(list(
    confidence = confidence,
    level = level,
    factors = list(
      base_similarity = suggestion$similarity,
      method_reliability = method_acceptance,
      source_confidence = source_is_ai,
      connection_multiplicity = connection_count,
      causal_completeness = grepl("chain", suggestion$method),
      domain_specificity = grepl("domain", suggestion$method)
    )
  ))
}

# Update suggestion card UI to show confidence
create_suggestion_card_ui <- function(ns, suggestion, index, suggestion_type) {
  confidence_info <- suggestion$confidence_info

  # Choose badge color based on confidence level
  badge_color <- case_when(
    confidence_info$level == "very_high" ~ "success",
    confidence_info$level == "high" ~ "info",
    confidence_info$level == "medium" ~ "warning",
    TRUE ~ "secondary"
  )

  # Confidence icon
  confidence_icon <- case_when(
    confidence_info$level == "very_high" ~ "fa-check-circle",
    confidence_info$level == "high" ~ "fa-thumbs-up",
    confidence_info$level == "medium" ~ "fa-info-circle",
    TRUE ~ "fa-question-circle"
  )

  div(
    class = "card mb-2 suggestion-card",
    div(
      class = "card-body p-2",

      # Confidence badge
      tags$span(
        class = paste0("badge bg-", badge_color),
        tags$i(class = paste("fas", confidence_icon)),
        " ",
        sprintf("Confidence: %.0f%%", confidence_info$confidence * 100)
      ),

      # Rest of card content...
    )
  )
}
```

**Benefits:**
- Users can prioritize high-confidence suggestions
- Reduces cognitive load (focus on best suggestions)
- Transparency in AI reasoning
- Basis for future ML models

**Success Metrics:**
- >90% acceptance rate for "very high" confidence
- 70-80% acceptance for "high" confidence
- <40% acceptance for "low" confidence (correctly identified weak suggestions)

---

### üöÄ P1: Performance Optimizations

#### I-004: Similarity Matrix Caching
**Priority**: P1 (High)
**Effort**: Medium (2 weeks)
**Impact**: Very High

**Description:**
Cache computed similarities to avoid recomputation. Critical for large vocabularies.

**Implementation:**
```r
# Global similarity cache (persists across sessions)
similarity_cache <- new.env()

# Cache key generator
get_cache_key <- function(text1, text2, method) {
  # Sort texts to make cache symmetric
  texts <- sort(c(text1, text2))
  key <- paste(texts[1], texts[2], method, sep = "|||")
  return(key)
}

# Cached similarity calculator
calculate_semantic_similarity_cached <- function(text1, text2, method = "jaccard",
                                                 use_cache = TRUE) {
  if (use_cache) {
    cache_key <- get_cache_key(text1, text2, method)

    # Check cache
    if (exists(cache_key, envir = similarity_cache)) {
      return(get(cache_key, envir = similarity_cache))
    }

    # Compute
    similarity <- calculate_semantic_similarity(text1, text2, method)

    # Store in cache
    assign(cache_key, similarity, envir = similarity_cache)

    return(similarity)
  } else {
    return(calculate_semantic_similarity(text1, text2, method))
  }
}

# Pre-compute similarity matrix on app startup
precompute_similarity_matrix <- function(vocabulary_data, methods = c("jaccard", "cosine")) {
  cat("üîÑ Pre-computing similarity matrix...\n")

  # Get all vocabulary items
  all_items <- rbind(
    data.frame(id = vocabulary_data$activities$id,
               name = vocabulary_data$activities$name,
               type = "Activity"),
    data.frame(id = vocabulary_data$pressures$id,
               name = vocabulary_data$pressures$name,
               type = "Pressure"),
    data.frame(id = vocabulary_data$consequences$id,
               name = vocabulary_data$consequences$name,
               type = "Consequence"),
    data.frame(id = vocabulary_data$controls$id,
               name = vocabulary_data$controls$name,
               type = "Control")
  )

  total_comparisons <- 0
  start_time <- Sys.time()

  # Compute all pairwise similarities
  for (i in 1:(nrow(all_items) - 1)) {
    for (j in (i + 1):nrow(all_items)) {
      # Only compute for different types
      if (all_items$type[i] != all_items$type[j]) {
        for (method in methods) {
          calculate_semantic_similarity_cached(
            all_items$name[i],
            all_items$name[j],
            method,
            use_cache = TRUE
          )
          total_comparisons <- total_comparisons + 1
        }
      }
    }

    # Progress indicator
    if (i %% 10 == 0) {
      cat(sprintf("  Progress: %d/%d items processed\r", i, nrow(all_items)))
    }
  }

  elapsed <- difftime(Sys.time(), start_time, units = "secs")

  cat(sprintf("\n‚úÖ Pre-computed %d similarities in %.2f seconds\n",
              total_comparisons, elapsed))
  cat(sprintf("   Cache size: %d entries\n", length(ls(similarity_cache))))

  # Save cache to disk for persistence
  saveRDS(as.list(similarity_cache), "cache/similarity_cache.rds")
}

# Load cache on startup
load_similarity_cache <- function() {
  cache_file <- "cache/similarity_cache.rds"

  if (file.exists(cache_file)) {
    cat("üì¶ Loading similarity cache from disk...\n")
    tryCatch({
      cached_data <- readRDS(cache_file)
      list2env(cached_data, envir = similarity_cache)
      cat(sprintf("‚úÖ Loaded %d cached similarities\n", length(cached_data)))
    }, error = function(e) {
      cat("‚ö†Ô∏è Failed to load cache:", e$message, "\n")
    })
  } else {
    cat("‚ÑπÔ∏è No cached similarities found - will compute on demand\n")
  }
}
```

**Benefits:**
- 10-100x speedup for repeated analyses
- Instant suggestions in workflow
- Supports real-time interactive features
- Reduces CPU load on server

**Performance Comparison:**
```
Without cache:
- 200 vocabulary items
- 19,900 comparisons
- ~15 seconds per analysis

With cache (warm):
- 200 vocabulary items
- 0 computations (all cached)
- ~0.2 seconds per analysis
- 75x speedup!
```

**Success Metrics:**
- Cache hit rate >95% after warmup
- Suggestion generation <300ms
- Memory usage <50MB for 500 items

---

#### I-005: Inverted Index for Keyword Matching
**Priority**: P1 (High)
**Effort**: Medium (2 weeks)
**Impact**: High

**Description:**
Build inverted index for O(1) keyword lookups instead of O(n) scans.

**Implementation:**
```r
# Build inverted index on startup
build_keyword_index <- function(vocabulary_data, themes = ENVIRONMENTAL_THEMES) {
  cat("üîç Building keyword index...\n")

  index <- new.env()

  # Get all items
  all_items <- rbind(
    data.frame(id = vocabulary_data$activities$id,
               name = vocabulary_data$activities$name,
               type = "Activity"),
    data.frame(id = vocabulary_data$pressures$id,
               name = vocabulary_data$pressures$name,
               type = "Pressure"),
    data.frame(id = vocabulary_data$consequences$id,
               name = vocabulary_data$consequences$name,
               type = "Consequence"),
    data.frame(id = vocabulary_data$controls$id,
               name = vocabulary_data$controls$name,
               type = "Control")
  )

  # For each theme
  for (theme_name in names(themes)) {
    theme <- themes[[theme_name]]
    theme_keywords <- theme$keywords

    # Pattern for all keywords
    pattern <- paste(theme_keywords, collapse = "|")

    # Find matching items
    matching_items <- all_items[grepl(pattern, tolower(all_items$name)), ]

    # Store in index
    if (nrow(matching_items) > 0) {
      assign(theme_name, matching_items, envir = index)
    }
  }

  cat(sprintf("‚úÖ Indexed %d themes covering %d items\n",
              length(ls(index)),
              sum(sapply(ls(index), function(k) nrow(get(k, envir = index))))))

  return(index)
}

# Fast keyword-based lookup
find_keyword_connections_indexed <- function(vocabulary_data,
                                            selected_items,
                                            target_type,
                                            keyword_index,
                                            themes = ENVIRONMENTAL_THEMES) {

  keyword_links <- data.frame()

  # Get selected item IDs
  selected_ids <- sapply(selected_items, function(x) x$id)

  # For each theme
  for (theme_name in names(themes)) {
    theme <- themes[[theme_name]]

    # Fast lookup from index
    if (exists(theme_name, envir = keyword_index)) {
      theme_items <- get(theme_name, envir = keyword_index)

      # Filter to selected sources and target type
      relevant_pairs <- expand.grid(
        from = selected_ids,
        to = theme_items$id[theme_items$type == target_type]
      )

      if (nrow(relevant_pairs) > 0) {
        # Only create links if source is in theme
        source_in_theme <- theme_items$id %in% selected_ids

        if (any(source_in_theme)) {
          for (i in 1:nrow(relevant_pairs)) {
            from_item <- theme_items[theme_items$id == relevant_pairs$from[i], ]
            to_item <- theme_items[theme_items$id == relevant_pairs$to[i], ]

            if (nrow(from_item) > 0 && nrow(to_item) > 0 &&
                from_item$type[1] != to_item$type[1]) {

              keyword_links <- rbind(keyword_links, data.frame(
                from_id = from_item$id[1],
                from_name = from_item$name[1],
                from_type = from_item$type[1],
                to_id = to_item$id[1],
                to_name = to_item$name[1],
                to_type = to_item$type[1],
                similarity = theme$strength,
                method = paste("keyword", theme_name, sep = "_"),
                stringsAsFactors = FALSE
              ))
            }
          }
        }
      }
    }
  }

  return(keyword_links)
}
```

**Benefits:**
- O(1) keyword lookups vs O(n) scans
- 50-100x faster for keyword matching
- Scales to thousands of vocabulary items
- Lower memory usage

**Performance Comparison:**
```
Without index (sequential scan):
- 500 items √ó 10 themes
- 5,000 regex operations
- ~2 seconds

With index:
- 10 hash lookups
- 0 regex operations
- ~0.02 seconds
- 100x speedup!
```

---

#### I-006: Parallel Processing for Large Vocabularies
**Priority**: P1 (High)
**Effort**: Large (3-4 weeks)
**Impact**: High

**Description:**
Use parallel processing for similarity computations on multi-core systems.

**Implementation:**
```r
# Parallel similarity computation
library(parallel)
library(foreach)
library(doParallel)

find_semantic_connections_parallel <- function(vocabulary_data,
                                              method = "jaccard",
                                              threshold = 0.3,
                                              n_cores = NULL) {

  if (is.null(n_cores)) {
    n_cores <- max(1, detectCores() - 1)  # Leave one core free
  }

  cat(sprintf("‚ö° Using %d cores for parallel processing\n", n_cores))

  # Prepare all items
  all_items <- rbind(
    data.frame(id = vocabulary_data$activities$id,
               name = vocabulary_data$activities$name,
               type = "Activity"),
    data.frame(id = vocabulary_data$pressures$id,
               name = vocabulary_data$pressures$name,
               type = "Pressure"),
    data.frame(id = vocabulary_data$consequences$id,
               name = vocabulary_data$consequences$name,
               type = "Consequence"),
    data.frame(id = vocabulary_data$controls$id,
               name = vocabulary_data$controls$name,
               type = "Control")
  )

  # Create comparison pairs (only different types)
  pairs <- data.frame()
  for (i in 1:(nrow(all_items) - 1)) {
    for (j in (i + 1):nrow(all_items)) {
      if (all_items$type[i] != all_items$type[j]) {
        pairs <- rbind(pairs, data.frame(idx1 = i, idx2 = j))
      }
    }
  }

  cat(sprintf("Processing %d comparison pairs...\n", nrow(pairs)))

  # Setup parallel backend
  cl <- makeCluster(n_cores)
  registerDoParallel(cl)

  # Export necessary objects to workers
  clusterExport(cl, c("calculate_semantic_similarity", "preprocess_text", "all_items"))

  # Parallel computation
  start_time <- Sys.time()

  results <- foreach(i = 1:nrow(pairs), .combine = rbind,
                     .packages = c("dplyr")) %dopar% {
    idx1 <- pairs$idx1[i]
    idx2 <- pairs$idx2[i]

    similarity <- calculate_semantic_similarity(
      all_items$name[idx1],
      all_items$name[idx2],
      method = method
    )

    if (similarity >= threshold) {
      data.frame(
        from_id = all_items$id[idx1],
        from_name = all_items$name[idx1],
        from_type = all_items$type[idx1],
        to_id = all_items$id[idx2],
        to_name = all_items$name[idx2],
        to_type = all_items$type[idx2],
        similarity = similarity,
        method = method,
        stringsAsFactors = FALSE
      )
    } else {
      NULL
    }
  }

  # Cleanup
  stopCluster(cl)

  elapsed <- difftime(Sys.time(), start_time, units = "secs")

  cat(sprintf("‚úÖ Parallel processing completed in %.2f seconds\n", elapsed))
  cat(sprintf("   Found %d connections above threshold\n", nrow(results)))

  return(results)
}
```

**Benefits:**
- Near-linear scaling with CPU cores
- 4x speedup on 4-core system
- 8x speedup on 8-core system
- Essential for vocabularies >1000 items

**Performance Comparison:**
```
1000 vocabulary items = ~500,000 comparisons

Single-core:
- Time: ~120 seconds
- CPU: 1 core at 100%

4-core parallel:
- Time: ~32 seconds
- CPU: 4 cores at 95%
- Speedup: 3.75x

8-core parallel:
- Time: ~17 seconds
- CPU: 8 cores at 93%
- Speedup: 7x
```

---

### üß† P2: Advanced NLP & Machine Learning

#### I-007: Word Embeddings (Word2Vec/GloVe)
**Priority**: P2 (Medium)
**Effort**: Large (4-5 weeks)
**Impact**: Very High

**Description:**
Use pre-trained word embeddings for semantic similarity instead of simple word overlap.

**Implementation:**
```r
# Load pre-trained embeddings (environmental domain-specific)
library(word2vec)
library(udpipe)

# Train custom embeddings on environmental corpus
train_environmental_embeddings <- function(corpus_files) {
  cat("üìö Training environmental word embeddings...\n")

  # Combine all environmental texts
  corpus <- c()
  for (file in corpus_files) {
    corpus <- c(corpus, readLines(file))
  }

  # Train Word2Vec model
  model <- word2vec(
    x = corpus,
    dim = 100,          # 100-dimensional embeddings
    iter = 20,
    min_count = 3,
    threads = 4,
    type = "cbow",      # or "skip-gram"
    window = 5
  )

  saveRDS(model, "models/environmental_embeddings.rds")

  cat("‚úÖ Trained embeddings on", length(corpus), "documents\n")

  return(model)
}

# Load embeddings
load_environmental_embeddings <- function() {
  model_file <- "models/environmental_embeddings.rds"

  if (file.exists(model_file)) {
    cat("üì¶ Loading environmental embeddings...\n")
    model <- readRDS(model_file)
    return(model)
  } else {
    cat("‚ö†Ô∏è No embeddings found - using basic similarity\n")
    return(NULL)
  }
}

# Embedding-based similarity
calculate_embedding_similarity <- function(text1, text2, embedding_model) {

  if (is.null(embedding_model)) {
    return(calculate_semantic_similarity(text1, text2))
  }

  # Tokenize
  tokens1 <- tolower(unlist(strsplit(text1, "\\s+")))
  tokens2 <- tolower(unlist(strsplit(text2, "\\s+")))

  # Get embeddings for each token
  embeddings1 <- predict(embedding_model, tokens1, type = "embedding")
  embeddings2 <- predict(embedding_model, tokens2, type = "embedding")

  # Average embeddings (simple but effective)
  avg_emb1 <- colMeans(embeddings1, na.rm = TRUE)
  avg_emb2 <- colMeans(embeddings2, na.rm = TRUE)

  # Cosine similarity
  similarity <- sum(avg_emb1 * avg_emb2) /
    (sqrt(sum(avg_emb1^2)) * sqrt(sum(avg_emb2^2)))

  return(similarity)
}
```

**Benefits:**
- Captures semantic meaning beyond word overlap
- Understands synonyms: "pollution" ‚âà "contamination"
- Contextual understanding: "discharge" in water vs. medical
- Domain-specific: trained on environmental texts

**Example Improvements:**
```
Simple overlap similarity:
"marine pollution" vs "ocean contamination" = 0.0 (no common words!)

Embedding similarity:
"marine pollution" vs "ocean contamination" = 0.85 (high semantic similarity)
```

---

#### I-008: Machine Learning Classification
**Priority**: P2 (Medium)
**Effort**: Large (6-8 weeks)
**Impact**: Very High

**Description:**
Train ML classifier to predict link quality based on features + user feedback.

**Implementation:**
```r
library(caret)
library(randomForest)

# Extract features from link
extract_link_features <- function(from_item, to_item, vocabulary_data) {

  features <- list(
    # Text-based features
    word_overlap = length(intersect(
      strsplit(tolower(from_item$name), "\\s+")[[1]],
      strsplit(tolower(to_item$name), "\\s+")[[1]]
    )),

    text_length_from = nchar(from_item$name),
    text_length_to = nchar(to_item$name),

    # Type features
    from_type = from_item$type,
    to_type = to_item$type,

    # Vocabulary level features
    from_level = if ("level" %in% names(from_item)) from_item$level else 2,
    to_level = if ("level" %in% names(to_item)) to_item$level else 2,

    # Keyword features (binary: has keyword from each theme)
    has_water_from = grepl("water|aquatic|marine", tolower(from_item$name)),
    has_water_to = grepl("water|aquatic|marine", tolower(to_item$name)),
    has_pollution_from = grepl("pollution|contamination|discharge", tolower(from_item$name)),
    has_pollution_to = grepl("pollution|contamination|discharge", tolower(to_item$name)),
    # ... (repeat for all 10 themes)

    # Causal indicators
    has_causal_words_from = grepl("causes|leads|results|triggers", tolower(from_item$name)),
    has_causal_words_to = grepl("loss|damage|degradation|impact", tolower(to_item$name)),

    # Similarity scores
    jaccard_similarity = calculate_semantic_similarity(from_item$name, to_item$name, "jaccard"),
    cosine_similarity = calculate_semantic_similarity(from_item$name, to_item$name, "cosine"),

    # Network features (if available)
    from_degree = get_node_degree(from_item$id, vocabulary_data),  # How connected is source?
    to_degree = get_node_degree(to_item$id, vocabulary_data)       # How connected is target?
  )

  return(as.data.frame(features))
}

# Train classifier
train_link_classifier <- function(feedback_data, vocabulary_data) {
  cat("ü§ñ Training ML classifier for link prediction...\n")

  # Prepare training data
  training_data <- data.frame()

  for (i in 1:nrow(feedback_data)) {
    row <- feedback_data[i, ]

    # Get full item data
    from_item <- get_vocabulary_item(row$from_id, vocabulary_data)
    to_item <- get_vocabulary_item(row$to_id, vocabulary_data)

    # Extract features
    features <- extract_link_features(from_item, to_item, vocabulary_data)

    # Add target variable (accepted = 1, rejected = 0)
    features$accepted <- ifelse(row$action == "accepted", 1, 0)

    training_data <- rbind(training_data, features)
  }

  cat(sprintf("  Training on %d examples\n", nrow(training_data)))
  cat(sprintf("  Positive class: %d (%.1f%%)\n",
              sum(training_data$accepted),
              100 * mean(training_data$accepted)))

  # Split into train/test
  set.seed(42)
  train_idx <- createDataPartition(training_data$accepted, p = 0.8, list = FALSE)
  train_set <- training_data[train_idx, ]
  test_set <- training_data[-train_idx, ]

  # Train Random Forest
  model <- randomForest(
    accepted ~ .,
    data = train_set,
    ntree = 500,
    mtry = sqrt(ncol(train_set) - 1),
    importance = TRUE
  )

  # Evaluate
  predictions <- predict(model, test_set, type = "response")
  accuracy <- mean((predictions > 0.5) == test_set$accepted)

  cat(sprintf("‚úÖ Model accuracy: %.2f%%\n", 100 * accuracy))

  # Feature importance
  importance <- importance(model)
  top_features <- head(importance[order(-importance[, "MeanDecreaseGini"]), ], 10)

  cat("\nüìä Top 10 most important features:\n")
  print(top_features)

  # Save model
  saveRDS(model, "models/link_classifier.rds")

  return(model)
}

# Use classifier for predictions
predict_link_quality <- function(from_item, to_item, vocabulary_data, classifier) {

  features <- extract_link_features(from_item, to_item, vocabulary_data)

  # Predict probability
  probability <- predict(classifier, features, type = "response")

  return(probability)
}
```

**Benefits:**
- Learns complex patterns from data
- Combines multiple signals optimally
- Continuously improves with more feedback
- Can discover new linking patterns

**Expected Performance:**
```
After 500+ feedback examples:
- Accuracy: 85-90%
- Precision: 80-85% (suggestions accepted)
- Recall: 75-80% (good links found)
- F1 Score: 77-82%

vs. Current rule-based:
- Precision: ~60-70%
- Recall: ~50-60%
```

---

### üåç P3: Domain Knowledge Expansion

#### I-009: Geographic/Regional Specialization
**Priority**: P3 (Medium)
**Effort**: Medium (3 weeks)
**Impact**: Medium

**Description:**
Add region-specific environmental rules (Mediterranean, Arctic, Tropical, etc.).

**Implementation:**
```r
# Regional environmental themes
REGIONAL_THEMES <- list(

  mediterranean = list(
    name = "Mediterranean Sea",
    keywords = c("mediterranean", "adriatic", "aegean", "tyrrhenian",
                "seagrass", "posidonia", "monk seal", "bluefin tuna"),
    specific_rules = list(
      # Specific pressures
      pressures = c("overfishing", "coastal tourism", "ship traffic", "plastic pollution"),
      # Specific consequences
      consequences = c("seagrass loss", "endemic species decline", "beach erosion"),
      # Specific controls
      controls = c("marine protected areas", "fishing quotas", "beach management")
    ),
    strength_multiplier = 1.2  # 20% boost for regional matches
  ),

  arctic = list(
    name = "Arctic Region",
    keywords = c("arctic", "polar", "ice", "permafrost", "tundra",
                "polar bear", "arctic fox", "bowhead whale"),
    specific_rules = list(
      pressures = c("ice loss", "permafrost thaw", "shipping lanes", "oil exploration"),
      consequences = c("habitat loss", "species migration", "coastal erosion"),
      controls = c("ice monitoring", "protected corridors", "emission reduction")
    ),
    strength_multiplier = 1.2
  ),

  tropical_reef = list(
    name = "Tropical Coral Reefs",
    keywords = c("coral", "reef", "tropical", "atoll", "lagoon",
                "coral bleaching", "reef fish", "mangrove"),
    specific_rules = list(
      pressures = c("ocean warming", "acidification", "sedimentation", "blast fishing"),
      consequences = c("coral bleaching", "reef degradation", "fishery collapse"),
      controls = c("reef restoration", "temperature monitoring", "fishing bans")
    ),
    strength_multiplier = 1.2
  ),

  # Add more regions: Baltic, North Sea, Pacific, Atlantic, Indian Ocean, etc.
)

# Region detection
detect_region <- function(selected_items, regional_themes = REGIONAL_THEMES) {

  # Extract all text
  all_text <- paste(
    sapply(selected_items, function(x) tolower(x$name)),
    collapse = " "
  )

  # Check each region
  region_scores <- list()

  for (region_name in names(regional_themes)) {
    region <- regional_themes[[region_name]]

    # Count keyword matches
    matches <- sum(sapply(region$keywords, function(kw) grepl(kw, all_text)))

    region_scores[[region_name]] <- matches
  }

  # Get top region
  if (length(region_scores) > 0 && max(unlist(region_scores)) > 0) {
    top_region <- names(which.max(region_scores))
    score <- region_scores[[top_region]]

    if (score >= 2) {  # At least 2 keyword matches
      return(list(
        region = top_region,
        confidence = min(1, score / 5),  # Normalize to 0-1
        theme = regional_themes[[top_region]]
      ))
    }
  }

  return(NULL)
}

# Apply regional boost to suggestions
enhance_suggestions_with_regional_knowledge <- function(suggestions,
                                                       selected_items,
                                                       regional_themes = REGIONAL_THEMES) {

  # Detect region
  detected_region <- detect_region(selected_items, regional_themes)

  if (is.null(detected_region)) {
    return(suggestions)  # No regional enhancement
  }

  cat(sprintf("üåç Detected region: %s (confidence: %.2f)\n",
              detected_region$theme$name,
              detected_region$confidence))

  # Apply boost to relevant suggestions
  for (i in 1:length(suggestions)) {
    suggestion <- suggestions[[i]]

    # Check if suggestion matches regional keywords
    suggestion_text <- tolower(suggestion$to_name)

    keyword_match <- any(sapply(
      detected_region$theme$keywords,
      function(kw) grepl(kw, suggestion_text)
    ))

    if (keyword_match) {
      # Apply regional boost
      boost <- detected_region$theme$strength_multiplier * detected_region$confidence
      suggestions[[i]]$similarity <- min(1, suggestion$similarity * boost)
      suggestions[[i]]$reasoning <- paste0(
        suggestions[[i]]$reasoning,
        " [Regional match: ", detected_region$theme$name, "]"
      )
    }
  }

  # Re-sort by updated similarity
  suggestions <- suggestions[order(sapply(suggestions, function(x) -x$similarity))]

  return(suggestions)
}
```

**Benefits:**
- More relevant suggestions for specific regions
- Captures regional environmental challenges
- Educational value (users learn regional issues)
- Expandable to any geography

---

#### I-010: Temporal/Seasonal Analysis
**Priority**: P3 (Low)
**Effort**: Medium (2-3 weeks)
**Impact**: Medium

**Description:**
Consider temporal aspects: seasonal patterns, climate trends, time-sensitive risks.

**Implementation:**
```r
# Temporal environmental patterns
TEMPORAL_PATTERNS <- list(

  seasonal = list(
    summer = list(
      months = c(6, 7, 8),
      enhanced_pressures = c("tourism", "water demand", "heat stress", "algal blooms"),
      enhanced_consequences = c("water scarcity", "ecosystem stress", "beach overcrowding")
    ),

    winter = list(
      months = c(12, 1, 2),
      enhanced_pressures = c("storm damage", "flooding", "heating emissions"),
      enhanced_consequences = c("coastal erosion", "habitat damage", "air pollution")
    ),

    spring = list(
      months = c(3, 4, 5),
      enhanced_pressures = c("agricultural runoff", "spawning disturbance", "fertilizer use"),
      enhanced_consequences = c("eutrophication", "reproductive failure", "algal blooms")
    ),

    autumn = list(
      months = c(9, 10, 11),
      enhanced_pressures = c("harvest activities", "storm preparation", "migration barriers"),
      enhanced_consequences = c("sediment runoff", "species displacement")
    )
  ),

  climate_trends = list(
    rising_temperature = list(
      keywords = c("warming", "heat", "temperature increase", "climate change"),
      enhanced_pressures = c("thermal stress", "bleaching", "species migration", "ice loss"),
      enhanced_consequences = c("habitat shift", "range contraction", "phenology mismatch"),
      strength_multiplier = 1.15
    ),

    sea_level_rise = list(
      keywords = c("sea level", "coastal", "inundation", "submersion"),
      enhanced_pressures = c("coastal flooding", "saltwater intrusion", "erosion"),
      enhanced_consequences = c("wetland loss", "habitat submersion", "displacement"),
      strength_multiplier = 1.15
    ),

    ocean_acidification = list(
      keywords = c("acidification", "pH", "carbonate", "shell", "calcification"),
      enhanced_pressures = c("reduced calcification", "shell dissolution"),
      enhanced_consequences = c("shellfish decline", "reef degradation", "food web disruption"),
      strength_multiplier = 1.2
    )
  )
)

# Apply temporal context
apply_temporal_context <- function(suggestions, current_month = NULL) {

  if (is.null(current_month)) {
    current_month <- as.numeric(format(Sys.Date(), "%m"))
  }

  # Determine season
  season <- if (current_month %in% c(6, 7, 8)) {
    "summer"
  } else if (current_month %in% c(12, 1, 2)) {
    "winter"
  } else if (current_month %in% c(3, 4, 5)) {
    "spring"
  } else {
    "autumn"
  }

  season_data <- TEMPORAL_PATTERNS$seasonal[[season]]

  cat(sprintf("üìÖ Applying %s seasonal context\n", season))

  # Boost seasonally relevant suggestions
  for (i in 1:length(suggestions)) {
    suggestion <- suggestions[[i]]
    suggestion_text <- tolower(suggestion$to_name)

    # Check for seasonal relevance
    is_seasonal <- any(sapply(
      season_data$enhanced_pressures,
      function(pattern) grepl(pattern, suggestion_text)
    )) || any(sapply(
      season_data$enhanced_consequences,
      function(pattern) grepl(pattern, suggestion_text)
    ))

    if (is_seasonal) {
      suggestions[[i]]$similarity <- min(1, suggestion$similarity * 1.1)  # 10% boost
      suggestions[[i]]$reasoning <- paste0(
        suggestions[[i]]$reasoning,
        " [Seasonally relevant: ", season, "]"
      )
    }
  }

  return(suggestions)
}
```

**Benefits:**
- Time-relevant risk assessment
- Seasonal planning support
- Climate change awareness
- Dynamic adaptation to current conditions

---

### üìä P4: Analytics & Visualization

#### I-011: Network Visualization Dashboard
**Priority**: P4 (Low)
**Effort**: Large (4 weeks)
**Impact**: Medium

**Description:**
Interactive network visualization showing all connections and their strengths.

**Implementation:**
```r
library(visNetwork)
library(igraph)

# Create interactive network visualization
create_link_network_viz <- function(links, vocabulary_data) {

  # Prepare nodes
  all_items <- rbind(
    data.frame(id = vocabulary_data$activities$id,
               label = vocabulary_data$activities$name,
               type = "Activity",
               group = "Activity",
               color = "#8E44AD"),
    data.frame(id = vocabulary_data$pressures$id,
               label = vocabulary_data$pressures$name,
               type = "Pressure",
               group = "Pressure",
               color = "#E74C3C"),
    data.frame(id = vocabulary_data$consequences$id,
               label = vocabulary_data$consequences$name,
               type = "Consequence",
               group = "Consequence",
               color = "#E67E22"),
    data.frame(id = vocabulary_data$controls$id,
               label = vocabulary_data$controls$name,
               type = "Control",
               group = "Control",
               color = "#27AE60")
  )

  # Filter to nodes that have links
  linked_nodes <- unique(c(links$from_id, links$to_id))
  nodes <- all_items[all_items$id %in% linked_nodes, ]

  # Prepare edges
  edges <- data.frame(
    from = links$from_id,
    to = links$to_id,
    value = links$similarity * 10,  # Width
    title = sprintf("%s ‚Üí %s<br>Similarity: %.2f<br>Method: %s",
                   links$from_name, links$to_name,
                   links$similarity, links$method),
    arrows = "to",
    color = ifelse(grepl("causal", links$method), "#3498db", "#95a5a6")
  )

  # Create visualization
  visNetwork(nodes, edges, width = "100%", height = "600px") %>%
    visNodes(shape = "dot", size = 20) %>%
    visEdges(smooth = TRUE) %>%
    visGroups(groupname = "Activity", color = "#8E44AD") %>%
    visGroups(groupname = "Pressure", color = "#E74C3C") %>%
    visGroups(groupname = "Consequence", color = "#E67E22") %>%
    visGroups(groupname = "Control", color = "#27AE60") %>%
    visLegend(width = 0.1, position = "right") %>%
    visOptions(
      highlightNearest = list(enabled = TRUE, degree = 1, hover = TRUE),
      nodesIdSelection = TRUE,
      manipulation = FALSE
    ) %>%
    visInteraction(navigationButtons = TRUE) %>%
    visLayout(randomSeed = 42)
}
```

**UI Integration:**
```r
# In UI
tabPanel(
  "üï∏Ô∏è Link Network",

  fluidRow(
    column(12,
      h3("AI-Generated Link Network"),
      p("Interactive visualization of vocabulary connections discovered by AI")
    )
  ),

  fluidRow(
    column(12,
      visNetworkOutput("link_network_viz", height = "600px")
    )
  ),

  fluidRow(
    column(4,
      selectInput("viz_method_filter", "Filter by Method",
                 choices = c("All" = "all", "Causal" = "causal",
                           "Keyword" = "keyword", "Semantic" = "jaccard"))
    ),
    column(4,
      sliderInput("viz_similarity_threshold", "Similarity Threshold",
                 min = 0, max = 1, value = 0.3, step = 0.05)
    ),
    column(4,
      actionButton("regenerate_viz", "üîÑ Regenerate Network",
                  class = "btn-primary")
    )
  )
)
```

**Benefits:**
- Visual understanding of relationships
- Interactive exploration
- Pattern identification
- Quality assessment at scale

---

#### I-012: Suggestion Analytics Dashboard
**Priority**: P4 (Low)
**Effort**: Medium (2-3 weeks)
**Impact**: Low

**Description:**
Dashboard showing suggestion performance metrics over time.

**Implementation:**
```r
# Analytics summary
create_suggestion_analytics <- function(feedback_data) {

  analytics <- list(

    # Overall metrics
    overall = list(
      total_suggestions = nrow(feedback_data),
      accepted = sum(feedback_data$action == "accepted"),
      rejected = sum(feedback_data$action == "rejected"),
      acceptance_rate = mean(feedback_data$action == "accepted")
    ),

    # By method
    by_method = feedback_data %>%
      group_by(method) %>%
      summarise(
        count = n(),
        accepted = sum(action == "accepted"),
        acceptance_rate = mean(action == "accepted"),
        avg_similarity = mean(similarity),
        .groups = 'drop'
      ) %>%
      arrange(desc(acceptance_rate)),

    # By link type
    by_link_type = feedback_data %>%
      mutate(link_type = paste(from_type, to_type, sep = " ‚Üí ")) %>%
      group_by(link_type) %>%
      summarise(
        count = n(),
        accepted = sum(action == "accepted"),
        acceptance_rate = mean(action == "accepted"),
        .groups = 'drop'
      ) %>%
      arrange(desc(acceptance_rate)),

    # By similarity band
    by_similarity = feedback_data %>%
      mutate(
        similarity_band = cut(similarity,
                             breaks = seq(0, 1, 0.1),
                             labels = paste0(seq(0, 90, 10), "-", seq(10, 100, 10), "%"))
      ) %>%
      group_by(similarity_band) %>%
      summarise(
        count = n(),
        accepted = sum(action == "accepted"),
        acceptance_rate = mean(action == "accepted"),
        .groups = 'drop'
      ),

    # Time series
    over_time = feedback_data %>%
      mutate(date = as.Date(timestamp)) %>%
      group_by(date) %>%
      summarise(
        suggestions = n(),
        accepted = sum(action == "accepted"),
        acceptance_rate = mean(action == "accepted"),
        .groups = 'drop'
      ) %>%
      arrange(date)
  )

  return(analytics)
}

# Visualization
plot_suggestion_analytics <- function(analytics) {

  library(ggplot2)
  library(plotly)

  # Acceptance rate by method
  p1 <- ggplot(analytics$by_method, aes(x = reorder(method, acceptance_rate),
                                         y = acceptance_rate,
                                         fill = method)) +
    geom_col() +
    geom_text(aes(label = sprintf("%.1f%%", 100 * acceptance_rate)),
             hjust = -0.1) +
    coord_flip() +
    scale_y_continuous(labels = scales::percent, limits = c(0, 1.1)) +
    labs(title = "Suggestion Acceptance Rate by Method",
         x = "Method",
         y = "Acceptance Rate") +
    theme_minimal() +
    theme(legend.position = "none")

  # Acceptance over time
  p2 <- ggplot(analytics$over_time, aes(x = date, y = acceptance_rate)) +
    geom_line(color = "#3498db", size = 1) +
    geom_point(color = "#3498db", size = 2) +
    geom_smooth(method = "loess", se = TRUE, color = "#e74c3c") +
    scale_y_continuous(labels = scales::percent) +
    labs(title = "Suggestion Acceptance Rate Over Time",
         x = "Date",
         y = "Acceptance Rate",
         subtitle = "Blue line = actual, Red line = trend") +
    theme_minimal()

  # Similarity calibration curve
  p3 <- ggplot(analytics$by_similarity, aes(x = similarity_band,
                                            y = acceptance_rate,
                                            fill = acceptance_rate)) +
    geom_col() +
    geom_text(aes(label = sprintf("%.1f%%", 100 * acceptance_rate)),
             vjust = -0.5) +
    scale_fill_gradient2(low = "#e74c3c", mid = "#f39c12", high = "#27ae60",
                        midpoint = 0.5) +
    scale_y_continuous(labels = scales::percent, limits = c(0, 1.1)) +
    labs(title = "Calibration Curve: Similarity vs Acceptance",
         x = "Similarity Range",
         y = "Acceptance Rate") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          legend.position = "none")

  return(list(by_method = ggplotly(p1),
             over_time = ggplotly(p2),
             calibration = ggplotly(p3)))
}
```

**Benefits:**
- Data-driven improvement decisions
- Identify underperforming methods
- Track system improvement over time
- Validate algorithm changes

---

### üîß P5: Configuration & Customization

#### I-013: User-Adjustable Thresholds
**Priority**: P5 (Low)
**Effort**: Small (1 week)
**Impact**: Low

**Description:**
Allow users to adjust suggestion sensitivity and preferences.

**Implementation:**
```r
# UI for user preferences
suggestion_preferences_ui <- function(ns) {
  div(
    class = "card",
    div(
      class = "card-header",
      h5("‚öôÔ∏è AI Suggestion Preferences")
    ),
    div(
      class = "card-body",

      sliderInput(
        ns("ai_similarity_threshold"),
        "Similarity Threshold (lower = more suggestions)",
        min = 0.1, max = 0.7, value = 0.3, step = 0.05
      ),

      sliderInput(
        ns("ai_max_suggestions"),
        "Maximum Suggestions Per Step",
        min = 3, max = 10, value = 5, step = 1
      ),

      checkboxGroupInput(
        ns("ai_methods"),
        "Enabled Methods",
        choices = c(
          "Causal Analysis" = "causal",
          "Keyword Matching" = "keyword",
          "Semantic Similarity" = "jaccard"
        ),
        selected = c("causal", "keyword", "jaccard")
      ),

      checkboxInput(
        ns("ai_show_low_confidence"),
        "Show Low Confidence Suggestions",
        value = FALSE
      ),

      radioButtons(
        ns("ai_sort_by"),
        "Sort Suggestions By",
        choices = c(
          "Similarity Score" = "similarity",
          "Confidence Score" = "confidence",
          "Alphabetical" = "alpha"
        ),
        selected = "confidence"
      ),

      actionButton(
        ns("reset_ai_preferences"),
        "Reset to Defaults",
        class = "btn-secondary btn-sm"
      )
    )
  )
}

# Apply user preferences
apply_user_preferences <- function(suggestions, preferences) {

  # Filter by methods
  suggestions <- suggestions[sapply(suggestions, function(s) {
    any(sapply(preferences$ai_methods, function(m) grepl(m, s$method)))
  })]

  # Filter by confidence if enabled
  if (!preferences$ai_show_low_confidence) {
    suggestions <- suggestions[sapply(suggestions, function(s) {
      s$confidence_info$level %in% c("high", "very_high")
    })]
  }

  # Sort
  if (preferences$ai_sort_by == "similarity") {
    suggestions <- suggestions[order(sapply(suggestions, function(s) -s$similarity))]
  } else if (preferences$ai_sort_by == "confidence") {
    suggestions <- suggestions[order(sapply(suggestions, function(s) -s$confidence_info$confidence))]
  } else {
    suggestions <- suggestions[order(sapply(suggestions, function(s) s$to_name))]
  }

  # Limit number
  suggestions <- head(suggestions, preferences$ai_max_suggestions)

  return(suggestions)
}
```

**Benefits:**
- User control and transparency
- Adapts to user expertise level
- Reduces suggestion fatigue
- Personalized experience

---

#### I-014: Custom Environmental Rules
**Priority**: P5 (Low)
**Effort**: Large (4-5 weeks)
**Impact**: Medium

**Description:**
Allow users/admins to define custom linking rules for specific domains.

**Implementation:**
```r
# Custom rule structure
custom_rule <- list(
  name = "Aquaculture Specific Rule",
  description = "Links aquaculture activities to specific pressures",

  conditions = list(
    from_type = "Activity",
    from_keywords = c("aquaculture", "fish farm", "shellfish farm"),
    to_type = "Pressure"
  ),

  target_keywords = c("organic waste", "chemical use", "disease", "escapees"),

  strength = 0.85,

  enabled = TRUE,

  created_by = "admin",
  created_at = Sys.time()
)

# Rule manager
custom_rules <- reactiveVal(list())

# Add custom rule
add_custom_rule <- function(rule) {
  rules <- custom_rules()
  rules <- c(rules, list(rule))
  custom_rules(rules)

  # Save to file
  saveRDS(rules, "config/custom_rules.rds")

  showNotification(
    paste("‚úÖ Custom rule added:", rule$name),
    type = "message"
  )
}

# Apply custom rules
apply_custom_rules <- function(links, selected_items, target_type, vocabulary_data) {

  rules <- custom_rules()

  if (length(rules) == 0) {
    return(links)  # No custom rules
  }

  custom_links <- data.frame()

  for (rule in rules) {
    if (!rule$enabled) next

    # Check if rule applies
    if (rule$conditions$to_type != target_type) next

    # Find matching source items
    source_items <- selected_items[sapply(selected_items, function(item) {
      item$type == rule$conditions$from_type &&
      any(sapply(rule$conditions$from_keywords, function(kw) {
        grepl(kw, tolower(item$name))
      }))
    })]

    if (length(source_items) == 0) next

    # Find matching target items
    target_vocab <- vocabulary_data[[tolower(paste0(target_type, "s"))]]

    target_items <- target_vocab[sapply(1:nrow(target_vocab), function(i) {
      any(sapply(rule$target_keywords, function(kw) {
        grepl(kw, tolower(target_vocab$name[i]))
      }))
    }), ]

    if (nrow(target_items) == 0) next

    # Create links
    for (source in source_items) {
      for (i in 1:nrow(target_items)) {
        custom_links <- rbind(custom_links, data.frame(
          from_id = source$id,
          from_name = source$name,
          from_type = source$type,
          to_id = target_items$id[i],
          to_name = target_items$name[i],
          to_type = target_type,
          similarity = rule$strength,
          method = paste("custom_rule", rule$name, sep = "_"),
          stringsAsFactors = FALSE
        ))
      }
    }
  }

  # Merge with existing links
  all_links <- rbind(links, custom_links)

  # Remove duplicates, keeping higher similarity
  all_links <- all_links %>%
    group_by(from_id, to_id) %>%
    slice_max(similarity, n = 1) %>%
    ungroup()

  return(all_links)
}

# UI for rule management
custom_rules_ui <- function(ns) {
  tagList(
    h4("üìã Custom Linking Rules"),

    actionButton(ns("add_custom_rule"), "‚ûï Add New Rule",
                class = "btn-primary mb-3"),

    DTOutput(ns("custom_rules_table"))
  )
}
```

**Benefits:**
- Domain-specific customization
- Organizational knowledge capture
- Flexibility for specialized use cases
- Community-contributed rules

---

## Priority Roadmap

### üöÄ Phase 1: Foundation (Q1 2026) - 3 months
**Focus**: User feedback and performance

- ‚úÖ **I-001**: Suggestion acceptance tracking
- ‚úÖ **I-002**: Adaptive similarity thresholds
- ‚úÖ **I-003**: Confidence scoring system
- ‚úÖ **I-004**: Similarity matrix caching
- ‚úÖ **I-005**: Inverted index for keywords

**Deliverables**:
- Database schema for feedback
- Adaptive threshold algorithm
- Cache system with persistence
- 10x performance improvement

**Success Criteria**:
- Suggestion acceptance rate >60%
- Suggestion generation <500ms
- Cache hit rate >95%

---

### üß† Phase 2: Intelligence (Q2 2026) - 3 months
**Focus**: Advanced NLP and ML

- ‚úÖ **I-007**: Word embeddings (Word2Vec)
- ‚úÖ **I-008**: ML classification model
- ‚úÖ **I-006**: Parallel processing

**Deliverables**:
- Environmental word embeddings
- Trained link classifier
- Parallel processing engine

**Success Criteria**:
- Embedding similarity >0.7 for synonyms
- Classifier accuracy >85%
- 4x speedup on multi-core systems

---

### üåç Phase 3: Domain Expansion (Q3 2026) - 2 months
**Focus**: Domain knowledge and specialization

- ‚úÖ **I-009**: Geographic/regional rules
- ‚úÖ **I-010**: Temporal/seasonal analysis

**Deliverables**:
- 10+ regional theme sets
- Seasonal adjustment system
- Climate trend integration

**Success Criteria**:
- Regional suggestion boost working
- Seasonal relevance >70% accuracy

---

### üìä Phase 4: Analytics & UX (Q4 2026) - 2 months
**Focus**: Visualization and user control

- ‚úÖ **I-011**: Network visualization
- ‚úÖ **I-012**: Analytics dashboard
- ‚úÖ **I-013**: User preferences

**Deliverables**:
- Interactive network viz
- Performance dashboard
- User preference system

**Success Criteria**:
- Dashboard used by >80% of users
- Preference customization >50% adoption

---

### üîß Phase 5: Customization (Q1 2027) - 2 months
**Focus**: Extensibility and custom rules

- ‚úÖ **I-014**: Custom environmental rules

**Deliverables**:
- Rule management system
- Rule sharing/export
- Community rule repository

**Success Criteria**:
- >20 community-contributed rules
- Custom rules used in >30% of projects

---

## Success Metrics

### Quantitative Metrics

**Accuracy Metrics:**
- Suggestion acceptance rate: >60% ‚Üí >80%
- Precision (relevant suggestions): >70% ‚Üí >85%
- Recall (missed good links): <30% ‚Üí <15%
- F1 Score: >0.65 ‚Üí >0.82

**Performance Metrics:**
- Suggestion generation time: <1s ‚Üí <300ms
- Cache hit rate: N/A ‚Üí >95%
- Parallel speedup: 1x ‚Üí 4-8x
- Memory usage: <100MB ‚Üí <50MB

**User Experience Metrics:**
- Workflow completion time: Baseline ‚Üí -40%
- User satisfaction: 3.5/5 ‚Üí 4.5/5
- Feature usage: Baseline ‚Üí +150%
- Return usage: Baseline ‚Üí +80%

### Qualitative Metrics

**User Feedback:**
- "Suggestions are more relevant"
- "Faster workflow completion"
- "Learned new environmental connections"
- "System understands my domain"

**System Quality:**
- Robust error handling
- Graceful degradation
- Clear explanations
- Transparent reasoning

---

## Implementation Priority Matrix

| Improvement | Priority | Effort | Impact | Risk | Quarter |
|-------------|----------|--------|--------|------|---------|
| I-001 Feedback tracking | P0 | M | VH | Low | Q1 |
| I-002 Adaptive thresholds | P0 | M | H | Low | Q1 |
| I-003 Confidence scoring | P0 | S | H | Low | Q1 |
| I-004 Similarity caching | P1 | M | VH | Low | Q1 |
| I-005 Keyword indexing | P1 | M | H | Low | Q1 |
| I-006 Parallel processing | P1 | L | H | Med | Q2 |
| I-007 Word embeddings | P2 | L | VH | Med | Q2 |
| I-008 ML classification | P2 | L | VH | Med | Q2 |
| I-009 Regional rules | P3 | M | M | Low | Q3 |
| I-010 Temporal analysis | P3 | M | M | Low | Q3 |
| I-011 Network viz | P4 | L | M | Low | Q4 |
| I-012 Analytics dashboard | P4 | M | L | Low | Q4 |
| I-013 User preferences | P5 | S | L | Low | Q4 |
| I-014 Custom rules | P5 | L | M | Med | Q1'27 |

**Legend:**
- Priority: P0=Critical, P1=High, P2=Medium, P3=Medium, P4=Low, P5=Low
- Effort: S=Small (1w), M=Medium (2-3w), L=Large (4-8w)
- Impact: VH=Very High, H=High, M=Medium, L=Low
- Risk: Low, Med, High

---

## Conclusion

This improvement roadmap provides a structured path to enhance the AI linkage system from its current production-ready state to a world-class intelligent suggestion engine. The phased approach balances quick wins (Phase 1) with long-term strategic investments (Phases 2-5).

**Key Takeaways:**

1. **Start with feedback** - User data is the foundation for all improvements
2. **Optimize early** - Performance gains enable advanced features
3. **Add intelligence gradually** - ML requires sufficient training data
4. **Expand domain knowledge** - Regional/temporal context increases relevance
5. **Empower users** - Analytics and customization drive adoption

**Expected Outcomes:**

By completing all phases, the system will:
- Achieve >80% suggestion acceptance rate
- Generate suggestions in <300ms
- Learn continuously from user behavior
- Adapt to specific domains and regions
- Provide transparent, explainable recommendations
- Scale to thousands of vocabulary items
- Support community contributions

---

**Document Status**: ‚úÖ Complete
**Next Review**: Q2 2026
**Maintained By**: AI Development Team
