# =============================================================================
# Advanced Performance Benchmark Tool for Environmental Bowtie Risk Analysis
# Version: 5.2.0 (Advanced Framework Edition)
# Author: Enhanced Development Framework
# Description: Comprehensive performance monitoring, regression detection, and optimization insights
# =============================================================================

# Load required libraries
suppressPackageStartupMessages({
  library(microbenchmark)
  library(profvis)
  library(pryr)
  library(memoise)
})

# Source application files for testing
cat("🔧 Loading application components for benchmarking...\n")
source("global.R")
source("utils.r")
source("vocabulary.r")

# =============================================================================
# Core Performance Benchmarks
# =============================================================================

run_performance_benchmarks <- function(include_regression_tests = TRUE,
                                       generate_report = TRUE,
                                       compare_baseline = TRUE) {
  cat("🚀 Starting Advanced Performance Benchmark Suite v5.2\n")
  cat("====================================================\n")

  # Enhanced memory and system monitoring
  initial_memory <- pryr::mem_used()
  system_info <- list(
    r_version = R.version.string,
    platform = Sys.info()[["sysname"]],
    cores = parallel::detectCores(),
    memory_gb = round(as.numeric(system("wmic computersystem get TotalPhysicalMemory /value", intern = TRUE)[2]) / 1024^3, 2)
  )

  cat("📊 System Information:\n")
  cat("   R Version:", system_info$r_version, "\n")
  cat("   Platform:", system_info$platform, "\n")
  cat("   CPU Cores:", system_info$cores, "\n")
  cat("   Initial Memory:", format(initial_memory, units = "Mb"), "\n\n")

  # Initialize results storage
  benchmark_results <- list()
  start_time <- Sys.time()

  # 1. Data Loading Performance
  cat("1️⃣ Data Loading Performance\n")
  data_loading_benchmark <- microbenchmark(
    vocabulary_loading = {
      activities <- read_vocabulary_data("CAUSES.xlsx", "Activities")
      pressures <- read_vocabulary_data("CAUSES.xlsx", "Pressures")
      consequences <- read_vocabulary_data("CONSEQUENCES.xlsx", "Consequences")
      controls <- read_vocabulary_data("CONTROLS.xlsx", "Controls")
    },
    times = 10
  )
  print(data_loading_benchmark)
  cat("\n")

  # 2. Bowtie Generation Performance
  cat("2️⃣ Bowtie Diagram Generation Performance\n")
  sample_data <- generate_sample_environmental_data()

  bowtie_benchmark <- microbenchmark(
    create_nodes = createBowtieNodesFixed(sample_data, "Sample Problem", 45, TRUE, TRUE),
    create_edges = createBowtieEdgesFixed(sample_data, TRUE),
    times = 20
  )
  print(bowtie_benchmark)
  cat("\n")

  # 3. Large Dataset Performance
  cat("3️⃣ Large Dataset Performance (1000 scenarios)\n")
  large_data <- generate_large_dataset(1000)

  large_dataset_benchmark <- microbenchmark(
    process_large_data = {
      nodes <- createBowtieNodesFixed(large_data, "Large Test Problem", 45, TRUE, TRUE)
      edges <- createBowtieEdgesFixed(large_data, TRUE)
    },
    times = 5
  )
  print(large_dataset_benchmark)
  cat("\n")

  # 4. Memory Usage Analysis
  cat("4️⃣ Memory Usage Analysis\n")
  final_memory <- pryr::mem_used()
  memory_increase <- final_memory - initial_memory
  cat("📈 Final memory usage:", format(final_memory, units = "Mb"), "\n")
  cat("📊 Memory increase:", format(memory_increase, units = "Mb"), "\n")

  # 5. Reactivity Performance (simulated)
  cat("\n5️⃣ Reactive Performance Simulation\n")
  reactive_benchmark <- microbenchmark(
    data_filtering = {
      filtered <- sample_data[sample_data$Central_Problem == "Sample Problem", ]
      unique_problems <- unique(filtered$Central_Problem)
    },
    data_transformation = {
      transformed <- transform_data_for_analysis(sample_data)
    },
    times = 50
  )
  print(reactive_benchmark)

  cat("\n✅ Performance benchmark completed successfully!\n")
  cat("📋 Summary saved to: performance_benchmark_results.txt\n")

  # Save results
  save_benchmark_results(data_loading_benchmark, bowtie_benchmark,
                        large_dataset_benchmark, reactive_benchmark,
                        initial_memory, final_memory)
}

# =============================================================================
# Helper Functions
# =============================================================================

generate_large_dataset <- function(n_scenarios = 1000) {
  cat("📊 Generating large dataset with", n_scenarios, "scenarios...\n")

  activities <- c("Industrial discharge", "Agricultural runoff", "Urban development",
                 "Transportation", "Mining operations")
  pressures <- c("Chemical pollution", "Nutrient loading", "Habitat destruction",
                "Physical disturbance", "Toxic contamination")
  consequences <- c("Biodiversity loss", "Water quality degradation", "Ecosystem collapse",
                   "Species extinction", "Habitat fragmentation")

  large_data <- data.frame(
    Activity = sample(activities, n_scenarios, replace = TRUE),
    Pressure = sample(pressures, n_scenarios, replace = TRUE),
    Preventive_Control = paste("Control", 1:n_scenarios),
    Escalation_Factor = paste("Factor", 1:n_scenarios),
    Central_Problem = "Large Test Problem",
    Protective_Mitigation = paste("Mitigation", 1:n_scenarios),
    Consequence = sample(consequences, n_scenarios, replace = TRUE),
    Consequence_Probability = runif(n_scenarios, 0.1, 0.9),
    Risk_Level = sample(c("Low", "Medium", "High", "Critical"), n_scenarios, replace = TRUE),
    stringsAsFactors = FALSE
  )

  cat("✅ Large dataset generated successfully\n")
  return(large_data)
}

transform_data_for_analysis <- function(data) {
  # Simulate data transformation operations
  data$processed_timestamp <- Sys.time()
  data$risk_score <- runif(nrow(data), 0, 100)
  data$category <- paste(data$Activity, data$Pressure, sep = " -> ")
  return(data)
}

save_benchmark_results <- function(data_loading, bowtie, large_dataset, reactive,
                                  initial_mem, final_mem) {
  results <- list(
    timestamp = Sys.time(),
    system_info = Sys.info(),
    r_version = R.version.string,
    memory_usage = list(
      initial = initial_mem,
      final = final_mem,
      increase = final_mem - initial_mem
    ),
    benchmarks = list(
      data_loading = data_loading,
      bowtie_generation = bowtie,
      large_dataset = large_dataset,
      reactive_operations = reactive
    )
  )

  # Save to RDS for detailed analysis
  saveRDS(results, "performance_benchmark_results.rds")

  # Save human-readable summary
  sink("performance_benchmark_results.txt")
  cat("Environmental Bowtie Risk Analysis - Performance Benchmark Results\n")
  cat("================================================================\n")
  cat("Timestamp:", as.character(Sys.time()), "\n")
  cat("R Version:", R.version.string, "\n")
  cat("System:", paste(Sys.info()[c("sysname", "release", "machine")], collapse = " "), "\n\n")

  cat("Memory Usage:\n")
  cat("- Initial:", format(initial_mem, units = "Mb"), "\n")
  cat("- Final:", format(final_mem, units = "Mb"), "\n")
  cat("- Increase:", format(final_mem - initial_mem, units = "Mb"), "\n\n")

  cat("Benchmark Results:\n")
  cat("==================\n")
  print(data_loading)
  cat("\n")
  print(bowtie)
  cat("\n")
  print(large_dataset)
  cat("\n")
  print(reactive)
  sink()
}

# =============================================================================
# Performance Profiling Functions
# =============================================================================

profile_application_startup <- function() {
  cat("🔍 Profiling application startup performance...\n")

  # Profile the startup process
  profvis({
    source("global.R")
    source("utils.r")
    source("vocabulary.r")
    source("guided_workflow.r")
    source("guided_workflow_steps.r")

    # Simulate data loading
    sample_data <- generate_sample_environmental_data()
    nodes <- createBowtieNodesFixed(sample_data, "Profile Test", 45, TRUE, TRUE)
    edges <- createBowtieEdgesFixed(sample_data, TRUE)
  }, interval = 0.01)
}

monitor_memory_usage <- function() {
  cat("📊 Monitoring memory usage patterns...\n")

  memory_log <- data.frame(
    step = character(),
    memory_mb = numeric(),
    timestamp = character(),
    stringsAsFactors = FALSE
  )

  # Function to log memory usage
  log_memory <- function(step_name) {
    mem_usage <- as.numeric(pryr::mem_used()) / 1024^2  # Convert to MB
    memory_log <<- rbind(memory_log, data.frame(
      step = step_name,
      memory_mb = mem_usage,
      timestamp = as.character(Sys.time()),
      stringsAsFactors = FALSE
    ))
    cat("📈", step_name, ":", round(mem_usage, 2), "MB\n")
  }

  log_memory("Initial")
  source("global.R"); log_memory("Global loaded")
  source("utils.r"); log_memory("Utils loaded")
  source("vocabulary.r"); log_memory("Vocabulary loaded")

  # Generate test data
  test_data <- generate_sample_environmental_data()
  log_memory("Sample data generated")

  # Create bowtie components
  nodes <- createBowtieNodesFixed(test_data, "Memory Test", 45, TRUE, TRUE)
  log_memory("Nodes created")

  edges <- createBowtieEdgesFixed(test_data, TRUE)
  log_memory("Edges created")

  # Clean up
  rm(nodes, edges, test_data)
  gc()
  log_memory("After cleanup")

  # Save memory log
  write.csv(memory_log, "memory_usage_log.csv", row.names = FALSE)
  cat("📋 Memory usage log saved to: memory_usage_log.csv\n")

  return(memory_log)
}

# =============================================================================
# Main Execution
# =============================================================================

if (!interactive()) {
  cat("🎯 Environmental Bowtie Risk Analysis - Performance Benchmark Tool\n")
  cat("==================================================================\n\n")

  # Install required packages if missing
  required_packages <- c("microbenchmark", "profvis", "pryr")
  missing_packages <- required_packages[!required_packages %in% installed.packages()[,"Package"]]

  if (length(missing_packages) > 0) {
    cat("📦 Installing missing packages:", paste(missing_packages, collapse = ", "), "\n")
    install.packages(missing_packages)
  }

  # Run benchmarks
  run_performance_benchmarks()

  # Monitor memory
  monitor_memory_usage()

  cat("\n🎉 Performance analysis complete!\n")
  cat("📁 Results saved in current directory\n")
} else {
  cat("📋 Performance benchmark tools loaded. Available functions:\n")
  cat("   - run_performance_benchmarks(): Run full benchmark suite\n")
  cat("   - profile_application_startup(): Profile startup performance\n")
  cat("   - monitor_memory_usage(): Monitor memory usage patterns\n")
  cat("   - generate_large_dataset(n): Generate large test dataset\n")
}